<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Bert Finetune on Sai Kaushik Soma</title><link>https://hugo-profile.netlify.app/tags/bert-finetune/</link><description>Recent content in Bert Finetune on Sai Kaushik Soma</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 04 Apr 2022 22:53:58 +0530</lastBuildDate><atom:link href="https://hugo-profile.netlify.app/tags/bert-finetune/index.xml" rel="self" type="application/rss+xml"/><item><title>Fine Tuning BERT</title><link>https://hugo-profile.netlify.app/blogs/bert_model/</link><pubDate>Mon, 04 Apr 2022 22:53:58 +0530</pubDate><guid>https://hugo-profile.netlify.app/blogs/bert_model/</guid><description>BERT is Bi Directional Encoder Transformer
BERT (Bi-Directional Encoder Representation from Transformers) is that type of transformer introduced by Google which consists of only encoder and no decoder.
BERT model comes in two sizes:
BERT BASE which has 12 encoder layers , 12 attention heads , 768 hidden layers and 110 million parameters BERT LARGE which has 24 encoder layers , 16 attention heads , 1024 hidden layers with 340 million parameters</description></item></channel></rss>
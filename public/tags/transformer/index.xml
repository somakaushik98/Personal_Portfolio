<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>transformer on Sai Kaushik Soma</title><link>https://hugo-profile.netlify.app/tags/transformer/</link><description>Recent content in transformer on Sai Kaushik Soma</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 03 Oct 2022 22:41:10 +0530</lastBuildDate><atom:link href="https://hugo-profile.netlify.app/tags/transformer/index.xml" rel="self" type="application/rss+xml"/><item><title>KeyPhraseTransformer</title><link>https://hugo-profile.netlify.app/blogs/keyphrase/</link><pubDate>Mon, 03 Oct 2022 22:41:10 +0530</pubDate><guid>https://hugo-profile.netlify.app/blogs/keyphrase/</guid><description>KeyPhraseTransformer is built on T5 Transformer architecture, trained on 500,000 training samples to extract important phrases/topics/themes from text of any length.
Why KeyPhraseTransformer? You get the power of amazing T5 architecture. The underlying T5 model is specifically trained in extracting important phrases from the text corpus, so the results are of superior quality. No pre-processing is needed of any kind. Just dump your data to the model It does not need any n-gram-related inputs from user.</description></item><item><title>Simple T5</title><link>https://hugo-profile.netlify.app/blogs/simplet5/</link><pubDate>Tue, 03 May 2022 23:29:21 +0530</pubDate><guid>https://hugo-profile.netlify.app/blogs/simplet5/</guid><description>SimpleT5 is built on top of PyTorch-lightning‚ö°Ô∏è and Transformersü§ó that lets you quickly train/fine-tune T5 models.
With simpleT5 ‚Äî It is very easy to fine-tune any T5 model on your dataset (Pandas dataframe )‚Äî for any task (summarization, translation, question-answering, or other sequence-to-sequence tasks), just ‚Äî import, instantiate, download a pre-trained model and train.
Before we jump on how to use simpleT5, a quick introduction about T5 ‚Äî
What is T5 ?</description></item></channel></rss>
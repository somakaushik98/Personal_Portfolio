<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>transformers on Sai Kaushik Soma</title><link>https://hugo-profile.netlify.app/tags/transformers/</link><description>Recent content in transformers on Sai Kaushik Soma</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 04 Apr 2022 22:53:58 +0530</lastBuildDate><atom:link href="https://hugo-profile.netlify.app/tags/transformers/index.xml" rel="self" type="application/rss+xml"/><item><title>Fine Tuning BERT</title><link>https://hugo-profile.netlify.app/blogs/bert_model/</link><pubDate>Mon, 04 Apr 2022 22:53:58 +0530</pubDate><guid>https://hugo-profile.netlify.app/blogs/bert_model/</guid><description>BERT is Bi Directional Encoder Transformer
BERT (Bi-Directional Encoder Representation from Transformers) is that type of transformer introduced by Google which consists of only encoder and no decoder.
BERT model comes in two sizes:
BERT BASE which has 12 encoder layers , 12 attention heads , 768 hidden layers and 110 million parameters BERT LARGE which has 24 encoder layers , 16 attention heads , 1024 hidden layers with 340 million parameters</description></item><item><title>GPT Fine Tuning</title><link>https://hugo-profile.netlify.app/blogs/gpt/</link><pubDate>Sun, 03 Apr 2022 19:53:33 +0530</pubDate><guid>https://hugo-profile.netlify.app/blogs/gpt/</guid><description>GPT (Generative Pretrained Transformer)
In this sense, we can say that the GPT-2 is basically the next word prediction feature of a keyboard app, but one that is much larger and more sophisticated than what your phone has. The GPT-2 was trained on a massive 40GB dataset called WebText that the OpenAI researchers crawled from the internet as part of the research effort. To compare in terms of storage size, the keyboard app I use, SwiftKey, takes up 78MBs of space.</description></item></channel></rss>
<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>openAI on Sai Kaushik Soma</title><link>https://hugo-profile.netlify.app/tags/openai/</link><description>Recent content in openAI on Sai Kaushik Soma</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 03 Apr 2022 19:53:33 +0530</lastBuildDate><atom:link href="https://hugo-profile.netlify.app/tags/openai/index.xml" rel="self" type="application/rss+xml"/><item><title>GPT Fine Tuning</title><link>https://hugo-profile.netlify.app/blogs/gpt/</link><pubDate>Sun, 03 Apr 2022 19:53:33 +0530</pubDate><guid>https://hugo-profile.netlify.app/blogs/gpt/</guid><description>GPT (Generative Pretrained Transformer)
In this sense, we can say that the GPT-2 is basically the next word prediction feature of a keyboard app, but one that is much larger and more sophisticated than what your phone has. The GPT-2 was trained on a massive 40GB dataset called WebText that the OpenAI researchers crawled from the internet as part of the research effort. To compare in terms of storage size, the keyboard app I use, SwiftKey, takes up 78MBs of space.</description></item></channel></rss>
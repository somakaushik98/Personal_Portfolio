<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>nlp on Sai Kaushik Soma</title><link>https://hugo-profile.netlify.app/tags/nlp/</link><description>Recent content in nlp on Sai Kaushik Soma</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 03 Oct 2022 22:41:10 +0530</lastBuildDate><atom:link href="https://hugo-profile.netlify.app/tags/nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>KeyPhraseTransformer</title><link>https://hugo-profile.netlify.app/blogs/keyphrase/</link><pubDate>Mon, 03 Oct 2022 22:41:10 +0530</pubDate><guid>https://hugo-profile.netlify.app/blogs/keyphrase/</guid><description>KeyPhraseTransformer is built on T5 Transformer architecture, trained on 500,000 training samples to extract important phrases/topics/themes from text of any length.
Why KeyPhraseTransformer? You get the power of amazing T5 architecture. The underlying T5 model is specifically trained in extracting important phrases from the text corpus, so the results are of superior quality. No pre-processing is needed of any kind. Just dump your data to the model It does not need any n-gram-related inputs from user.</description></item><item><title>Fine Tuning BERT</title><link>https://hugo-profile.netlify.app/blogs/bert_model/</link><pubDate>Mon, 04 Apr 2022 22:53:58 +0530</pubDate><guid>https://hugo-profile.netlify.app/blogs/bert_model/</guid><description>BERT is Bi Directional Encoder Transformer
BERT (Bi-Directional Encoder Representation from Transformers) is that type of transformer introduced by Google which consists of only encoder and no decoder.
BERT model comes in two sizes:
BERT BASE which has 12 encoder layers , 12 attention heads , 768 hidden layers and 110 million parameters BERT LARGE which has 24 encoder layers , 16 attention heads , 1024 hidden layers with 340 million parameters</description></item></channel></rss>
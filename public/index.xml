<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Sai Kaushik Soma</title><link>https://hugo-profile.netlify.app/</link><description>Recent content on Sai Kaushik Soma</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 03 Oct 2022 22:41:10 +0530</lastBuildDate><atom:link href="https://hugo-profile.netlify.app/index.xml" rel="self" type="application/rss+xml"/><item><title>KeyPhraseTransformer</title><link>https://hugo-profile.netlify.app/blogs/keyphrase/</link><pubDate>Mon, 03 Oct 2022 22:41:10 +0530</pubDate><guid>https://hugo-profile.netlify.app/blogs/keyphrase/</guid><description>KeyPhraseTransformer is built on T5 Transformer architecture, trained on 500,000 training samples to extract important phrases/topics/themes from text of any length.
Why KeyPhraseTransformer? You get the power of amazing T5 architecture. The underlying T5 model is specifically trained in extracting important phrases from the text corpus, so the results are of superior quality. No pre-processing is needed of any kind. Just dump your data to the model It does not need any n-gram-related inputs from user.</description></item><item><title>Certifications</title><link>https://hugo-profile.netlify.app/gallery/</link><pubDate>Sat, 25 Jun 2022 18:35:46 +0530</pubDate><guid>https://hugo-profile.netlify.app/gallery/</guid><description/></item><item><title>Simple T5</title><link>https://hugo-profile.netlify.app/blogs/simplet5/</link><pubDate>Tue, 03 May 2022 23:29:21 +0530</pubDate><guid>https://hugo-profile.netlify.app/blogs/simplet5/</guid><description>SimpleT5 is built on top of PyTorch-lightning‚ö°Ô∏è and Transformersü§ó that lets you quickly train/fine-tune T5 models.
With simpleT5 ‚Äî It is very easy to fine-tune any T5 model on your dataset (Pandas dataframe )‚Äî for any task (summarization, translation, question-answering, or other sequence-to-sequence tasks), just ‚Äî import, instantiate, download a pre-trained model and train.
Before we jump on how to use simpleT5, a quick introduction about T5 ‚Äî
What is T5 ?</description></item><item><title>Fine Tuning BERT</title><link>https://hugo-profile.netlify.app/blogs/bert_model/</link><pubDate>Mon, 04 Apr 2022 22:53:58 +0530</pubDate><guid>https://hugo-profile.netlify.app/blogs/bert_model/</guid><description>BERT is Bi Directional Encoder Transformer
BERT (Bi-Directional Encoder Representation from Transformers) is that type of transformer introduced by Google which consists of only encoder and no decoder.
BERT model comes in two sizes:
BERT BASE which has 12 encoder layers , 12 attention heads , 768 hidden layers and 110 million parameters BERT LARGE which has 24 encoder layers , 16 attention heads , 1024 hidden layers with 340 million parameters</description></item><item><title>GPT Fine Tuning</title><link>https://hugo-profile.netlify.app/blogs/gpt/</link><pubDate>Sun, 03 Apr 2022 19:53:33 +0530</pubDate><guid>https://hugo-profile.netlify.app/blogs/gpt/</guid><description>GPT (Generative Pretrained Transformer)
In this sense, we can say that the GPT-2 is basically the next word prediction feature of a keyboard app, but one that is much larger and more sophisticated than what your phone has. The GPT-2 was trained on a massive 40GB dataset called WebText that the OpenAI researchers crawled from the internet as part of the research effort. To compare in terms of storage size, the keyboard app I use, SwiftKey, takes up 78MBs of space.</description></item></channel></rss>